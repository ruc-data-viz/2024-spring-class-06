{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c0ca5d-8f3b-4d3d-b4ab-7691b4ebda27",
   "metadata": {},
   "source": [
    "# Correlation & (Linear) Fitting\n",
    "\n",
    "When working with more than one variable we can use various methods to quantify how well the variables correlate with one another. We meaure the correlation between two metrics using a scalar value inclusively between -1 and 1. This value is known as the *correlation coefficient*; values near +1 indicate a strong positive relation, values near -1 indicate a strong negative relation (inversely correlated), and values near or equal to 0 indicate weak or no relation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab23d3-5995-4006-a331-50deca0701c1",
   "metadata": {},
   "source": [
    "\n",
    "## Pearson's Correlation Coefficient\n",
    "\n",
    "This is the most commonly used correlation coefficient. It is simple to compute, and for us, as users of `numpy`, it is even easier (it is literally just a function call!).\n",
    "\n",
    "$$\n",
    "r = \\frac{\n",
    "        n\\Sigma{xy}-(\\Sigma{x})(\\Sigma{y})\n",
    "    }{\n",
    "        [\\sqrt{n\\Sigma{x^{2}}-(\\Sigma{x})^{2}][n\\Sigma{y^{2}}-(\\Sigma{y})^{2}}]\n",
    "    }\n",
    "$$\n",
    "\n",
    "where `r` is our correlation coefficient, `x` and `y` are our data, and `n` is the sample size of the data. It does not *look* too simple at first glance, but it is and is pretty easy to compute. Consider the following dataset showing absences vs final grades for a set of students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e1ea5-fa0e-4649-bebd-2820676a3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'absences': [0, 1, 1, 2, 3, 3, 4, 5, 6, 7],\n",
    "    'grade': [90, 85, 88, 84, 82, 80, 75, 60, 72, 64]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d37bfd-f864-4d0f-b338-fa3520dcd7dc",
   "metadata": {},
   "source": [
    "If we visualize this, we can quickly *see* the correlation between the two metrics, but just how correlated are they? Let's visualize out data and compute the correlation coefficient using the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de68e3-bdd3-4949-a3c0-9a697e42c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='absences', y='grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a264a3-2952-4e48-a526-d8c9f267a861",
   "metadata": {},
   "source": [
    "A quick reminder that we can reach into a data frame and grab particular columns. These columns are ultimately just `numpy` arrays, and so we can perform arithmetic operations quite easily on them! We are going create dummy variables `x`, `y`, and `n` to make write the code a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8a858-c172-40ef-9bde-8a54a3440ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "x = df.absences\n",
    "y = df.grade\n",
    "n, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a9c55-42a1-46d8-90b1-ef95771d5365",
   "metadata": {},
   "source": [
    "While `x` and `y` are specifically `pandas.Series` objects (and not `numpy.ndarry`), we can still utilize them as such, as they mimic the functionality provided by `numpy`. This means we can call functions like `sum` on them to compute their sum easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae09753-8cca-42bf-b934-2b5bddfc59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(), y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f66b9-040c-403b-8f06-a92db1e67578",
   "metadata": {},
   "source": [
    "Lastly, recall that we can apply exponents to values using the `**` operator, e.g. we can sqaure each element of `x` using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719686f-3f6b-4f8d-b124-54455bb01437",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f037040-88aa-4d1c-9d5d-7776bd63f348",
   "metadata": {},
   "source": [
    "With all of that situated, let's combine everything together and implement the equation for our correlation coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b846be-75de-4f14-b31d-d6850f8396ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "r = (n * (x * y).sum() - x.sum() * y.sum()) / np.sqrt((n * (x**2).sum() - x.sum()**2) * ((n*(y**2).sum() - y.sum()**2)))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c28113-b94b-4bfe-af6d-1c4975d2e0ad",
   "metadata": {},
   "source": [
    "The value of `-0.906` implies a strong negative correlation between the two variables (not necessarily causation, though in thise case there is grounds to believe causation). This makes sense - as a student misses school, their grades will likely suffer.\n",
    "\n",
    "But boo! Why should we need to manually compute this value with all of that code above, especially one that is allegedly so common and popular? Well, luckily `numpy` gives us a pretty easy way to compute this particular metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e5ccb-856a-4c2c-a972-3110c0256d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(df.absences, df.grade)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847da652-b7d7-4fc8-b4e7-2ae1d57d2739",
   "metadata": {},
   "source": [
    "The function [`numpy.corrcoeff`](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html) computes the correlation coefficients, and returns to us the *correlation matrix*, which shows the correlation between each variable against every variable, including itself. Each row of this matrix represents the coefficients each variable - the first row is how well it correlates to the set; the second row is how it correlates to the set; etc.The diagonals are 1 since any variable will be strongly correlated to itself. As this can be applied to more than just 2 variables at a time, the generalized form for computing this with `numpy` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242676b4-88d4-47e2-83e4-1472a8eca57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef([df.absences, df.grade])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb67849-3dd8-4deb-9891-89e7bc3d6b8f",
   "metadata": {},
   "source": [
    "Note the addition of the square braces. This denotes a *list* of metrics we want to compute the correlation coefficients for, and thus we are not limited to passing just two variables.\n",
    "\n",
    "There are other means of computing correlation, but for most purposes this is sufficient. Some reminders and things to note about Pearson's Correlation:\n",
    " \n",
    "* this does not indicate causation\n",
    "* we cannot determine independent/dependent variables\n",
    "* applicable only to linear relationships\n",
    "* can be misleading in small sample sizes\n",
    "* can be skewed due to clusters of data and outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8282c74-cb76-4ff5-8b31-987ad8aec54e",
   "metadata": {},
   "source": [
    "## Linear Fitting\n",
    "\n",
    "Another way we can assess correlation, and as discussed in reference to *Anscombe's Quartet*, is to produce and visualize linear trend lines overtop a scatter of the data. As we have briefly seen before, `numpy` also makes this easy. We are going to make use of `numpy`'s *Polynomial API*.\n",
    "\n",
    "`numpy` implements a very powerful and flexible [Polynomial API](https://numpy.org/doc/stable/reference/routines.polynomials.html) with many polynomial finding routines implemented. Using a spread of data points we can determine approximate polynomial fittings. For our purposes for looking at correlation, we really only need to use a least squares fit.\n",
    "\n",
    "We can start by using the `polyfit` function to determine the coefficients of our fitting polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8a92f-8ea7-47cd-bfa8-a66bf8b349da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.polynomial as polynomial\n",
    "coeffs = polynomial.polyfit(df.absences, df.grade, 1)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb2a92-7402-4834-86cd-923a13216c28",
   "metadata": {},
   "source": [
    "These coefficients are for a polynomial of the form:\n",
    "\n",
    "$$\n",
    "p(x) = c_{0} + c_{1}x + c_{2}x^{2} + ...\n",
    "$$\n",
    "\n",
    "With these coefficients, we can then use the `polyval` function to evaluate the polynomial with these coefficients at the bounds of out data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4471b-f5d6-4a2b-af17-0c9fe589bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = polynomial.polyval([0,7], coeffs)\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6196e-6a09-42de-b3e7-aa81c26b09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter('absences', 'grade')\n",
    "ax.plot([0,7], fit)\n",
    "ax.text(x=4, y=90, s=f'r = {r[0][1]:.3f}', fontsize='large')\n",
    "\n",
    "# create a nicely formatted polynomial\n",
    "poly = polynomial.Polynomial([_.round(2) for _ in coeffs])\n",
    "\n",
    "ax.text(x=4, y=85, s=f'f(x) $\\\\approx$ {poly}', fontsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455b4fc-45d7-41f3-8a84-62639fb5b43e",
   "metadata": {},
   "source": [
    "This matches our expectations as we can see a clear trend in the data reinforced with our linear fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db9869-5f64-484e-a784-e7ba7db87450",
   "metadata": {},
   "source": [
    "### Where Linear Corrleation & Fitting Does Not Work\n",
    "\n",
    "As mentioned, there are times when computing the correlation for a data set does not make sense. We are going to take a quick look at some examples of when we want to avoid looking at correlation and linear trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb57ac3-b981-4496-b4c7-873ca95e4c52",
   "metadata": {},
   "source": [
    "#### Nonlinear Data\n",
    "\n",
    "As correlation is a linear statistic, it is not applicable to use for non-linear data. This does not mean the variables being assessed are not correlated or related in some way, just that we cannot quantify that relationship with the methods learned. Consider quadratic data and its trendline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb24b8-7dcb-4077-8fd9-3a79448e911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic = pd.DataFrame({\n",
    "    'x': np.linspace(-5,5,25)\n",
    "})\n",
    "quadratic['y'] = quadratic.x**2\n",
    "ax = quadratic.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(quadratic.x, quadratic.y, 1)\n",
    "fit = polynomial.polyval([-5,5], coeffs)\n",
    "ax.plot([-5,5], fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca80f18-684e-4d09-a63b-59db568cfd7c",
   "metadata": {},
   "source": [
    "The trend line asserts that there is no variability within the data (slope == 0). Now let's compute Pearson's Correlation Coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80812ae8-c473-4f40-b96c-327f04437f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(quadratic.x, quadratic.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece8789-5c78-496b-b424-647b02ea627d",
   "metadata": {},
   "source": [
    "Here we have a corefficient that is essentially 0, though there very well could be a relationship at work here (one that just is not linear!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5744f-37b5-4557-84f3-b14963fff198",
   "metadata": {},
   "source": [
    "#### Small Sample Sizes\n",
    "\n",
    "This one is applicable to more than just linear correlation - any sample size that is too small is hard to use to draw meaningful conclusions. Consider the following contrived dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380402de-9283-43b0-ab35-5c7f3768323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small = pd.DataFrame({\n",
    "    'x': [1.0, 2.2, 3.8],\n",
    "    'y': [0.5, 2.2, 2.3]\n",
    "})\n",
    "ax = small.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(small.x, small.y, 1)\n",
    "fit = polynomial.polyval([1.0, 3.8], coeffs)\n",
    "ax.plot([1.0, 3.8], fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0dc56b-1af3-4cce-8e81-12fff885d65b",
   "metadata": {},
   "source": [
    "Now let's compute Pearson's Correlation Coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc3470-8f9f-4d30-bab5-2ec04f37ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(small.x, small.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b20849-7011-4321-8807-9e648bddca9f",
   "metadata": {},
   "source": [
    "With our limited data we are told that there is a strong correlation between our variables, but 3 data points is hardly enough to really understand the data. A single additional data point that is within the range of our current data could greatly swing our correlation coefficient in either direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e726c-52ad-4342-b948-0d7ef9fa105e",
   "metadata": {},
   "source": [
    "#### Clusters of Data\n",
    "\n",
    "Our data may consist of various clusters of points - these clusters themselves may or may not have trends within, but collectively may show a trend altogether that may be misleading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ca3de-ca26-4568-81a4-ab4361b80fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.DataFrame({\n",
    "    'x': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "    'y': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "})\n",
    "ax = clusters.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(clusters.x, clusters.y, 1)\n",
    "fit = polynomial.polyval([0.0, 5.5], coeffs)\n",
    "ax.plot([0.0, 5.5], fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a46dd6-28d9-4bb9-aa7a-25dbc5861057",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(clusters.x, clusters.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a2819-0d9b-4e90-8ba1-0ee9ec18b57f",
   "metadata": {},
   "source": [
    "There appears to be a very string correlation here in our data, but it is likely more useful to evaluate the individual clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061ab59-b2c4-4c6d-82bc-baec325d6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.DataFrame({\n",
    "    'x': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "    'y': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "})\n",
    "ax = clusters.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(clusters.x, clusters.y, 1)\n",
    "fit = polynomial.polyval([0.0, 5.5], coeffs)\n",
    "ax.plot([0.0, 5.5], fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d5627-99d0-42fb-a096-67fccef6a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(clusters.x, clusters.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7412ef-93e4-4552-bf5b-c2433e9db538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = clusters.iloc[:25]\n",
    "ax = cluster1.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(cluster1.x, cluster1.y, 1)\n",
    "fit = polynomial.polyval([0.0, 0.5], coeffs)\n",
    "ax.plot([0.0, 0.5], fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a76551-2e9a-4021-9a37-fa320952b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(cluster1.x, cluster1.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c1b57-22fc-400f-8814-ef1ce7dfa9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2 = clusters.iloc[25:]\n",
    "ax = cluster2.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(cluster2.x, cluster2.y, 1)\n",
    "fit = polynomial.polyval([5.0, 5.5], coeffs)\n",
    "ax.plot([5.0, 5.5], fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa661f7-0273-4d26-bf01-5e29a5b699b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(cluster2.x, cluster2.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ffa172-a096-402a-a17c-704088ba8a0e",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "Outliers in data can greatly skew measures of correlation within a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ffbeb-beef-477b-87f8-e4644b740afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = pd.DataFrame({\n",
    "    'x': np.random.uniform(0.0, 0.5, 25),\n",
    "    'y': np.random.uniform(0.0, 0.5, 25),\n",
    "})\n",
    "outlier.iloc[-1] = [5.0, 5.0]\n",
    "ax = outlier.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(outlier.x, outlier.y, 1)\n",
    "fit = polynomial.polyval([0.0, 5.0], coeffs)\n",
    "ax.plot([0.0, 5.0], fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0f638-ecc6-4bb6-b8d2-001ed12dbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(outlier.x, outlier.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a690a-7d8e-4463-966d-7b8bc6321034",
   "metadata": {},
   "source": [
    "Again our trendline and correlation coefficient indicate a strong relation in the data, but the single outlier at `(5.0, 5.0)` is skewing our assessment of the data. If we strip that value from our data, we see that there is not much to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ea6f9-3640-4c29-8b43-8faaf476ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_removed = outlier.iloc[:-1]\n",
    "ax = outlier_removed.plot.scatter('x', 'y')\n",
    "\n",
    "coeffs = polynomial.polyfit(outlier_removed.x, outlier_removed.y, 1)\n",
    "fit = polynomial.polyval([0.0, 0.5], coeffs)\n",
    "ax.plot([0.0, 0.5], fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828fbf1b-8b4d-47e5-b39b-974df71a25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(outlier_removed.x, outlier_removed.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d25735-a5a6-4333-89cb-22ed0c5fb4d5",
   "metadata": {},
   "source": [
    "This is a much more accurate representation of our data! However, it is worth noting that outliers in data are not necessarily bad. There very well could be something in your model or system that behaves outlandish under certain criteria, and while it may make some statistical operations and observations worse/less conclusive, it is always best to understand the reasons behind any and all outliers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
