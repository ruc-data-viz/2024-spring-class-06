{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccab23d3-5995-4006-a331-50deca0701c1",
   "metadata": {},
   "source": [
    "# Data, Correlation, & Fitting Data\n",
    "\n",
    "## Bivariate Data\n",
    "\n",
    "As the name implies, bivariate data is data that consists of two variables. We compare and analyze the two variables with respect to one another in order to attempt to find and/or explain the relationship between them. It is also possible that one of these variables depends on the other, in which case we have an independent variable and a dependent variable.\n",
    "\n",
    "* altitude and air density\n",
    "* ice cream sales and temperature throughout a day\n",
    "* mana cost of a MTG card and the turns remaining in a game\n",
    "\n",
    "When we have bivariate data, there are some simple things we can do to help us understand what sort of relationship our variables have with one another. We want to be able to visualize our data as well as visualize the relationship (if there is one) and quantify it. We can use a combination of `pandas`, `numpy`, and `matplotlib`/`seaborn`/`bokeh` to handle the analysis and visualization.\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "First let's consider data from the *U.S. Standard Atmostphere 1976*. We simply want to look at how the density of air changes as we move higher into the atmosphere. In our simple data set (a full one has been provided) consists of altitudes in meters and densities in kilograms per meters cubed.\n",
    "\n",
    "The original data and model can be found in its original form, provided by NASA [here](https://ntrs.nasa.gov/api/citations/19770009539/downloads/19770009539.pdf). We however are using a stripped down version sourced from [here](https://www.engineeringtoolbox.com/standard-atmosphere-d_604.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e1ea5-fa0e-4649-bebd-2820676a3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "atmos = pd.read_csv('data/atmosphere_simple.csv')\n",
    "atmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0219441-6f04-40be-9484-16b3ce30bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "# general note on hvPlot vs Holoviews - usually we will create a plot using hvPlot when we have a dataframe,\n",
    "# and then use Holoviews to manipulate and compose it.\n",
    "\n",
    "(atmos.hvplot.line(x='altitude', y='air_density') * atmos.hvplot.scatter(x='altitude', y='air_density')).opts(\n",
    "    xlabel='Altitude (m)',\n",
    "    ylabel='Density of Air ($kg/m^{3}$)',\n",
    "    title='U.S. Standard Atmosphere, Altitude vs. Density of Air',\n",
    "    xlim=(-5000, 85000),\n",
    "    height=480,\n",
    "    show_grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df93b4-bf21-4fe5-be74-0b9c1366730b",
   "metadata": {},
   "source": [
    "We can see that there is a pretty strong relationship between these two variables (in the direction that we expect - at higher altitudes air is much thinner!). We can see too that this relationship is non-linear - air density rapidly decreases as we ascend to about 20km in altitude. It seems to asymtotically approach a density of 0 as we continue upward (in reality it is not asymtotic, as it is around 100000m (100km), that atmospheric pressue, and thus the density of air, becomes 0).\n",
    "\n",
    "You may ask why did we plot altitude on the x-axis, when it makes more sense to represent it vertically? The answer is actually very simple: altitude in an indepenent variable in this context! The graph above tells us that as the altitude increases, the density of air decreases. Let's flip it and see what it loops like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(atmos.hvplot.line(x='air_density', y='altitude') * atmos.hvplot.scatter(x='air_density', y='altitude')).opts(\n",
    "    xlabel='Density of Air ($kg/m^{3}$)',\n",
    "    ylabel='Altitude (m)',\n",
    "    title='U.S. Standard Atmosphere, Density of Air vs. Altitude',\n",
    "    xlim=(-0.1, 1.4),\n",
    "    height=480,\n",
    "    show_grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ecd607",
   "metadata": {},
   "source": [
    "This swapped ordering is still technically valid as it still shows how the data is related, but now indicates that the changes in alitude are caused by changes in air density (which is not true!). We need to take care when selecting what data goes on specific axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6337787",
   "metadata": {},
   "source": [
    "## Multivariate Data\n",
    "\n",
    "As the name implies, multivariate data is data that consists of more than two variables. The idea is the same as it is with bivariate data, except we have more variables to work with.\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "Again let's consider data from the *U.S. Standard Atmostphere 1976*.\n",
    "\n",
    "The original data and model can be found in its original form, provided by NASA [here](https://ntrs.nasa.gov/api/citations/19770009539/downloads/19770009539.pdf). We however are using a stripped down version sourced from [here](https://www.engineeringtoolbox.com/standard-atmosphere-d_604.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ee1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "atmos = pd.read_csv('data/atmosphere.csv')\n",
    "atmos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf9f74",
   "metadata": {},
   "source": [
    "We can look at any two variables with respect to altitude (our independent variable!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d74065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "\n",
    "# it can get annoying to repeat a lot of common parameters when creating overlays; we can put such common\n",
    "# parameters in a dictionary, and then expand that dictionary whenever we want to use those parameters in\n",
    "# a function call\n",
    "plot_axes = {\n",
    "    'x': 'altitude',\n",
    "    'y': 'atmospheric_pressure',\n",
    "}\n",
    "\n",
    "altitude_vs_pressure = atmos.hvplot.line(**plot_axes) * atmos.hvplot.scatter(**plot_axes)\n",
    "\n",
    "altitude_vs_pressure.opts(\n",
    "    xlabel='Altitude (m)',\n",
    "    ylabel='Atmospheric Pressure ($$10^{-5}N s/m^{2}$$)',\n",
    "    title='Altitude vs. Atmospheric Pressure',\n",
    "    xlim=(-5000, 85000),\n",
    "    height=480,\n",
    "    show_grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eded0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_axes = {\n",
    "    'x': 'altitude',\n",
    "    'y': 'temperature',\n",
    "}\n",
    "\n",
    "altitude_vs_temperature = atmos.hvplot.line(**plot_axes) * atmos.hvplot.scatter(**plot_axes)\n",
    "\n",
    "altitude_vs_temperature.opts(\n",
    "    xlabel='Altitude (m)',\n",
    "    ylabel='Temeprature ($^{\\circ}C$)',\n",
    "    title='Altitude vs. Temperature',\n",
    "    xlim=(-5000, 85000),\n",
    "    height=480,\n",
    "    show_grid=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafcb830",
   "metadata": {},
   "source": [
    "Or we can plot them simulataneously on the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_axes = {\n",
    "    'x': 'altitude',\n",
    "    'y': ['atmospheric_pressure', 'temperature'],\n",
    "}\n",
    "\n",
    "altitude_vs_pressure_and_temperature = atmos.hvplot.line(**plot_axes) * atmos.hvplot.scatter(**plot_axes)\n",
    "\n",
    "altitude_vs_pressure_and_temperature.opts(\n",
    "    xlabel='Altitude (m)',\n",
    "    title='Altitude vs. Atmospheric Pressure & Temperature',\n",
    "    xlim=(-5000, 85000),\n",
    "    height=480,\n",
    "    show_grid=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639303b2",
   "metadata": {},
   "source": [
    "The problem with setting both of these variables on the same plot is that numerically they are within very different ranges of values. Temperature fluctuates from 20 down to nearly -80, and our density only ranges from a little over 10.0 to nearly 0.0.\n",
    "\n",
    "There are different ways to handle this, but ideally we can just use a separate y-axis on the right side of our plot. There is only light support of this outside of `matplotlib`, but we can make it work. THe biggest problem is that we need to compose the plots slightly more granuarly to ensure that enabling the twin-axis works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the first plot\n",
    "plot_axes = {\n",
    "    'x': 'altitude',\n",
    "    'y': 'atmospheric_pressure',\n",
    "}\n",
    "altitude_vs_pressure = atmos.hvplot.line(**plot_axes) * atmos.hvplot.scatter(**plot_axes)\n",
    "\n",
    "# compose the second plot\n",
    "plot_axes = {\n",
    "    'x': 'altitude',\n",
    "    'y': 'temperature',\n",
    "}\n",
    "altitude_vs_temperature = atmos.hvplot.line(**plot_axes) * atmos.hvplot.scatter(**plot_axes)\n",
    "\n",
    "# compoose everything together, and enable the twin-axis using the `multi_y` parameter\n",
    "(altitude_vs_temperature * altitude_vs_pressure).opts(\n",
    "    xlabel='Altitude (m)',\n",
    "    title='Altitude vs. Atmospheric Pressure & Temperature',\n",
    "    xlim=(-5000, 85000),\n",
    "    height=480,\n",
    "    show_grid=True,\n",
    "    multi_y=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752604a",
   "metadata": {},
   "source": [
    "Now we can see both data sets in better detail (specifically atmospheric pressure), as temperature is added to a secondary axis. This allows us to avoid visually squashing the pressure data. Throwing more data into this plot however would not work well, as we cannot add any more axes, and everything at varying scales becomes unmaintainable. A this point we would defer to composing multiple plots in a typcal layout without any overlays. We could also relay on using colors and size (as we have seen before) for representing multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70209a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "\n",
    "# compose the first plot, \n",
    "plot_axes = {\n",
    "    'x': 'altitude',\n",
    "    'y': 'atmospheric_pressure'\n",
    "}\n",
    "altitude_vs_pressure = atmos.hvplot.line(**plot_axes) * atmos.hvplot.scatter(**plot_axes, s='dynamic_viscosity').opts(size=hv.dim('dynamic_viscosity')**4)\n",
    "\n",
    "# compose the second plot\n",
    "plot_axes = {\n",
    "    'x': 'altitude',\n",
    "    'y': 'temperature',\n",
    "}\n",
    "altitude_vs_temperature = atmos.hvplot.line(**plot_axes) * atmos.hvplot.scatter(**plot_axes, s='dynamic_viscosity').opts(size=hv.dim('dynamic_viscosity')**4)\n",
    "\n",
    "# compoose everything together, and enable the twin-axis using the `multi_y` parameter\n",
    "(altitude_vs_temperature * altitude_vs_pressure).opts(\n",
    "    xlabel='Altitude (m)',\n",
    "    title='Altitude vs. Atmospheric Pressure & Temperature',\n",
    "    xlim=(-5000, 85000),\n",
    "    height=480,\n",
    "    show_grid=True,\n",
    "    multi_y=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837960f",
   "metadata": {},
   "source": [
    "There are many ways for us to work with and visualize multivariate data, but it is highly dependent on the data. Data that consists of clusters of data can be visualized using methods like parallel coordinate/Andrews Curves; data that consists of categorical data may be representable with a radar plot; and sometimes the data just needs to be manipulated in ways to reduce what an analyst is looking at. We will soon be looking in more detail on how to cluster and group data, especially when working with data that is not directly categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a3e29",
   "metadata": {},
   "source": [
    "## Correlation & (Linear) Fitting\n",
    "\n",
    "When working with more than one variable we can use various methods to quantify how well the variables correlate with one another. We meaure the correlation between two metrics using a scalar value inclusively between -1 and 1. This value is known as the *correlation coefficient*; values near +1 indicate a strong positive relation, values near -1 indicate a strong negative relation (inversely correlated), and values near or equal to 0 indicate weak or no relation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10291f1",
   "metadata": {},
   "source": [
    "### Pearson's Correlation Coefficient\n",
    "\n",
    "This is the most commonly used correlation coefficient. It is simple to compute, and for us, as users of `numpy`, it is even easier (it is literally just a function call!).\n",
    "\n",
    "$$\n",
    "r = \\frac{\n",
    "        n\\Sigma{xy}-(\\Sigma{x})(\\Sigma{y})\n",
    "    }{\n",
    "        [\\sqrt{n\\Sigma{x^{2}}-(\\Sigma{x})^{2}][n\\Sigma{y^{2}}-(\\Sigma{y})^{2}}]\n",
    "    }\n",
    "$$\n",
    "\n",
    "where `r` is our correlation coefficient, `x` and `y` are our data, and `n` is the sample size of the data. It does not *look* too simple at first glance, but it is and is pretty easy to compute. Consider the following dataset showing absences vs final grades for a set of students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ebe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'absences': [0, 1, 1, 2, 3, 3, 4, 5, 6, 7],\n",
    "    'grade': [90, 85, 88, 84, 82, 80, 75, 60, 72, 64]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.scatter(x='absences', y='grade', grid=True, title='Absences vs. Grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e6071",
   "metadata": {},
   "source": [
    "A quick reminder that we can reach into a data frame and grab particular columns. These columns are ultimately just `numpy` arrays, and so we can perform arithmetic operations quite easily on them! We are going create dummy variables `x`, `y`, and `n` to make write the code a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "x = df.absences\n",
    "y = df.grade\n",
    "n, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115515bb",
   "metadata": {},
   "source": [
    "Let's combine everything together and implement the equation for our correlation coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad995d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "r = (n * (x * y).sum() - x.sum() * y.sum()) / np.sqrt((n * (x**2).sum() - x.sum()**2) * ((n*(y**2).sum() - y.sum()**2)))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca7969",
   "metadata": {},
   "source": [
    "The value of `-0.906` implies a strong negative correlation between the two variables (not necessarily causation, though in thise case there is grounds to believe causation). This makes sense - as a student misses school, their grades will likely suffer.\n",
    "\n",
    "But boo! Why should we need to manually compute this value with all of that code above, especially one that is allegedly so common and popular? Well, luckily `numpy` gives us a pretty easy way to compute this particular metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faddbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(df.absences, df.grade)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c393d",
   "metadata": {},
   "source": [
    "The function [`numpy.corrcoeff`](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html) computes the correlation coefficients, and returns to us the *correlation matrix*, which shows the correlation between each variable against every variable, including itself. Each row of this matrix represents the coefficients each variable - the first row is how well it correlates to the set; the second row is how it correlates to the set; etc.The diagonals are 1 since any variable will be strongly correlated to itself. As this can be applied to more than just 2 variables at a time, the generalized form for computing this with `numpy` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef([df.absences, df.grade])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37de8aa",
   "metadata": {},
   "source": [
    "Note the addition of the square braces. This denotes a *list* of metrics we want to compute the correlation coefficients for, and thus we are not limited to passing just two variables.\n",
    "\n",
    "There are other means of computing correlation, but for most purposes this is sufficient. Some reminders and things to note about Pearson's Correlation:\n",
    " \n",
    "* this does not indicate causation\n",
    "* we cannot determine independent/dependent variables\n",
    "* applicable only to linear relationships\n",
    "* can be misleading in small sample sizes\n",
    "* can be skewed due to clusters of data and outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afceab7d",
   "metadata": {},
   "source": [
    "## Linear Fitting\n",
    "\n",
    "Another way we can assess correlation, and as discussed in reference to *Anscombe's Quartet*, is to produce and visualize linear trend lines overtop a scatter of the data. As we have briefly seen before, `numpy` also makes this easy. We are going to make use of `numpy`'s *Polynomial API*.\n",
    "\n",
    "`numpy` implements a very powerful and flexible [Polynomial API](https://numpy.org/doc/stable/reference/routines.polynomials.html) with many polynomial finding routines implemented. Using a spread of data points we can determine approximate polynomial fittings. For our purposes for looking at correlation, we really only need to use a least squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8844c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# load and prep data\n",
    "data = pd.read_csv('data/anscombes_quartet.csv')\n",
    "data.columns = pd.MultiIndex.from_product([['I', 'II', 'III', 'IV'], ['x', 'y']])\n",
    "\n",
    "# create linear regression for first subset\n",
    "fitted = Polynomial.fit(data['I'].x, data['I'].y, 1)\n",
    "xs = np.array(data['I'].x)\n",
    "ys = fitted(xs)\n",
    "\n",
    "# scatter points and plot linear regression\n",
    "scatter = hv.Scatter(data['I']).opts(size=5)\n",
    "curve = hv.Curve({'x': xs, 'y':ys}).opts(color='red')\n",
    "(scatter * curve).opts(height=480, width=700, show_grid=True, title=\"Anscombe's Quartet; Dataset I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79757592",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44777c1",
   "metadata": {},
   "source": [
    "These coefficients are for a polynomial of the form:\n",
    "\n",
    "$$\n",
    "p(x) = c_{0} + c_{1}x + c_{2}x^{2} + ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d18a47",
   "metadata": {},
   "source": [
    "### Where Linear Corrleation & Fitting Does Not Work\n",
    "\n",
    "As mentioned, there are times when computing the correlation for a data set does not make sense. We are going to take a quick look at some examples of when we want to avoid looking at correlation and linear trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89dba5",
   "metadata": {},
   "source": [
    "#### Nonlinear Data\n",
    "\n",
    "As correlation is a linear statistic, it is not applicable to use for non-linear data. This does not mean the variables being assessed are not correlated or related in some way, just that we cannot quantify that relationship with the methods learned. Consider quadratic data and its trendline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create non-linear data\n",
    "quadratic = pd.DataFrame({\n",
    "    'x': np.linspace(-5,5,25)\n",
    "})\n",
    "quadratic['y'] = quadratic.x**2\n",
    "\n",
    "# create linear regresison\n",
    "fitted = Polynomial.fit(quadratic.x, quadratic.y, 1)\n",
    "\n",
    "# plot data with linear regression\n",
    "scatter = hv.Scatter(quadratic).opts(size=5)\n",
    "curve = hv.Curve({'x': quadratic.x, 'y':fitted(quadratic.x)}).opts(color='red')\n",
    "(scatter * curve).opts(height=480, width=700, show_grid=True, title='Poor Linear Regression: Quadratic Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaeb2a0",
   "metadata": {},
   "source": [
    "The trend line asserts that there is no variability within the data (slope == 0). Now let's compute Pearson's Correlation Coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63519a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(quadratic.x, quadratic.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3328835",
   "metadata": {},
   "source": [
    "Here we have a coefficient that is essentially 0, though there very well could be a relationship at work here (one that just is not linear!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23304c24",
   "metadata": {},
   "source": [
    "#### Small Sample Sizes\n",
    "\n",
    "This one is applicable to more than just linear correlation - any sample size that is too small is hard to use to draw meaningful conclusions. Consider the following contrived dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small data\n",
    "small = pd.DataFrame({\n",
    "    'x': [1.0, 2.2, 3.8],\n",
    "    'y': [0.5, 2.2, 2.3]\n",
    "})\n",
    "\n",
    "# create linear regresison\n",
    "fitted = Polynomial.fit(small.x, small.y, 1)\n",
    "\n",
    "# plot data with linear regression\n",
    "scatter = hv.Scatter(small).opts(size=10)\n",
    "curve = hv.Curve({'x': small.x, 'y':fitted(small.x)}).opts(color='red')\n",
    "(scatter * curve).opts(height=480, width=700, show_grid=True, title='Poor Linear Regression: Small Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1015e5",
   "metadata": {},
   "source": [
    "Now let's compute Pearson's Correlation Coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(small.x, small.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5fd2c7",
   "metadata": {},
   "source": [
    "With our limited data we are told that there is a strong correlation between our variables, but 3 data points is hardly enough to really understand the data. A single additional data point that is within the range of our current data could greatly swing our correlation coefficient in either direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217ada5",
   "metadata": {},
   "source": [
    "#### Clusters of Data\n",
    "\n",
    "Our data may consist of various clusters of points - these clusters themselves may or may not have trends within, but collectively may show a trend altogether that may be misleading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc881519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters of data\n",
    "clusters = pd.DataFrame({\n",
    "    'x': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "    'y': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "})\n",
    "# create linear regresison\n",
    "fitted = Polynomial.fit(clusters.x, clusters.y, 1)\n",
    "\n",
    "# plot data with linear regression\n",
    "scatter = hv.Scatter(clusters).opts(size=5)\n",
    "curve = hv.Curve({'x': clusters.x, 'y':fitted(clusters.x)}).opts(color='red')\n",
    "(scatter * curve).opts(height=480, width=700, show_grid=True, title='Poor Linear Regression: Clustered Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(clusters.x, clusters.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d935599",
   "metadata": {},
   "source": [
    "There appears to be a very strong correlation here in our data, but it is likely more useful to evaluate the individual clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters of data\n",
    "clusters = pd.DataFrame({\n",
    "    'x': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "    'y': np.concatenate([np.random.uniform(0.0, 0.5, 25), np.random.uniform(5.0, 5.5, 25)]),\n",
    "})\n",
    "# create linear regresison\n",
    "fitted_1 = Polynomial.fit(clusters.iloc[:25].x, clusters.iloc[:25].y, 1)\n",
    "fitted_2 = Polynomial.fit(clusters.iloc[25:].x, clusters.iloc[25:].y, 1)\n",
    "\n",
    "scatter = hv.Scatter(clusters).opts(size=5)\n",
    "curve_1 = hv.Curve({'x': clusters.iloc[:25].x, 'y':fitted_1(clusters.iloc[:25].x)}).opts(color='red')\n",
    "curve_2 = hv.Curve({'x': clusters.iloc[25:].x, 'y':fitted_2(clusters.iloc[25:].x)}).opts(color='red')\n",
    "\n",
    "(scatter * curve_1 * curve_2).opts(height=480, width=700, show_grid=True, title='Better Linear Regression: Individual Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12100fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(clusters.iloc[:25].x, clusters.iloc[:25].y)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10642a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(clusters.iloc[25:].x, clusters.iloc[25:].y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4417c729",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "Outliers in data can greatly skew measures of correlation within a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters of data\n",
    "outlier = pd.DataFrame({\n",
    "    'x': np.random.uniform(0.0, 0.5, 25),\n",
    "    'y': np.random.uniform(0.0, 0.5, 25),\n",
    "})\n",
    "outlier.iloc[-1] = [5.0, 5.0]\n",
    "\n",
    "# create linear regresison\n",
    "fitted = Polynomial.fit(outlier.x, outlier.y, 1)\n",
    "\n",
    "# plot data with linear regression\n",
    "scatter = hv.Scatter(outlier).opts(size=5)\n",
    "curve = hv.Curve({'x': outlier.x, 'y':fitted(outlier.x)}).opts(color='red')\n",
    "(scatter * curve).opts(height=480, width=700, show_grid=True, title='Poor Linear Regression: Outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(outlier.x, outlier.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff36ac2",
   "metadata": {},
   "source": [
    "Again our trendline and correlation coefficient indicate a strong relation in the data, but the single outlier at `(5.0, 5.0)` is skewing our assessment of the data. If we strip that value from our data, we see that there is not much to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7de84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoec the outlier\n",
    "outlier_removed = outlier.iloc[:-1]\n",
    "\n",
    "# create linear regresison\n",
    "fitted = Polynomial.fit(outlier_removed.x, outlier_removed.y, 1)\n",
    "\n",
    "# plot data with linear regression\n",
    "scatter = hv.Scatter(outlier_removed).opts(size=5)\n",
    "curve = hv.Curve({'x': outlier_removed.x, 'y':fitted(outlier_removed.x)}).opts(color='red')\n",
    "(scatter * curve).opts(height=480, width=700, show_grid=True, title='Better Linear Regression: Outlier Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(outlier_removed.x, outlier_removed.y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59010d1a",
   "metadata": {},
   "source": [
    "This is a much more accurate representation of our data! However, it is worth noting that outliers in data are not necessarily bad. There very well could be something in your model or system that behaves outlandish under certain criteria, and while it may make some statistical operations and observations worse/less conclusive, it is always best to understand the reasons behind any and all outliers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd0cb4",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Below we have some noisy data artificially representing radar measurements of a cannonball. Create a second-degree polynomial fit of the data to estimate the true path of the cannonball!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102da309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate noisy data\n",
    "x = np.linspace(0, 10.0, 100)\n",
    "y = 49.05*x - 4.905 * x * x + np.random.normal(0.0, 5.0, 100)\n",
    "measurements = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "# plot noisy data\n",
    "scatter = hv.Scatter(measurements).opts(size=5)\n",
    "scatter.opts(show_grid=True, height=480, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "084644d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement and visualize the fit here!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
